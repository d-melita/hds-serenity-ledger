\documentclass{article}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{thmtools}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{float}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{framed}
\usepackage[dvipsnames]{xcolor}
\usepackage{tcolorbox}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}

% ------------------------------------------------------------------------------

\begin{document}

% ------------------------------------------------------------------------------
% Cover Page and ToC
% ------------------------------------------------------------------------------

\title{ \normalsize \textsc{}
		\\ [2.0cm]
		\HRule{1.5pt} \\
		\LARGE \textbf{Highly Dependable Systems
		\HRule{2.0pt} \\ [0.6cm] \LARGE{Project Report - First Delivery} \vspace*{10\baselineskip}}
		}
\date{March 8th 2024}
\author{\textbf{Authors} \\ 
		Carolina Coelho - 99189\\
		Diogo Melita - 99202\\
		Diogo Antunes - 99210}

%\includegraphics[scale=0.2]{ist-logo.png}
\maketitle
\newpage

% ------------------------------------------------------------------------------

\section{System Design}

This project implements a simplified permissioned blockchain system with high
dependability guarantees, that uses the Istanbul BFT consensus algorithm at the
consensus layer. The system is compose by $n$ replicas and a variable number of 
clients whose identity is known before-hand. Also, a Public Key Infrastructure (PKI) is
assumed to be in place and for this purpose the \texttt{PKI} can be used.

\subsection{Communication}

The preexisting code already provided reliable links (link that provided eventual
delivery with uniqueness guarantees). To accommodate for the existence of Byzantine
replicas in the system, these need to be extended to ensure authenticity as well (otherwise,
Byzantine nodes could attack the system by forging messages).
Clients and replicas could naively ensure authenticity by using the PKI
to digitaly sign each message. This, however, is computationally demanding
and not strictly required for the problem at hand. We found that the use of HMACs
for this purpose was a better choice. To setup the shared secret between a pair
of nodes, the PKI is leveraged - the protocol used is that for a given pair of nodes, 
the one with lowest id generates the shared secret and sends it encrypted with
the receipient's public key and signed with its own private key.
After the initial setup, the shared secret is used to send an HMAC with each 
message to ensure integrity and authenticity. However, the text is not encrypted
as its contents are intended to be public. The new class that implements this
abstraction is the \texttt{HMACLink}.

\subsection{Replica architecture}

Each replica node runs two service - the \texttt{HDSLedgerService} and the \texttt{NodeService} (both of which use the \texttt{HMACLink}).

\subsubsection{\texttt{HDSLedgerService}}

This service is provided by replicas to clients with a simple interface.
Replicas receive \texttt{AppendReques} messages and reply with \texttt{AppendReply}
messages. A request by a client indicates to the replica that it wishes to append
the provided value to the blockchain (the clients is also expected to provide a
sequence number to allow for deduplication). Once the replica has confirmation that
the value is in the blockchain, it replies to the client. Since there might exist
Byzantine nodes in the system, the replica must wait for $f + 1$ replies from
replicas (because the first $f$ might be Byzantine nodes lying).

\subsubsection{\texttt{NodeService}}

This service is used only by replicas to reach consensus by using QBFT. The \texttt{NodeService}
also provides an API used by \texttt{HDSLedgerService} to initiate consensus and being
notified of agreement of values (this uses the observer pattern). Under the hood the
\texttt{NodeService} has two main threads always running - a listener and a driver.
The listener is responsible for receiving messages from the network and redirecting
them to the correct consensus instance. The driver listens for consensus outputs and 
notifies the observers when an input is confirmed (and does so in the correct order).
Since the consensus ran in this service is what underlies SMR, the systems resilience
to Byzantine faults is the same as the underlying consensus protocol, i.e., 
it tolerates up to $\frac{n-1}{3}$ faults and requires and partially-synchronous network
for liveness. In other words, the blockchain will be a total order of all client requests.
Furthermore, if a correct client sends a request to all replicas, the value will eventually be appended to the blockchain.

\subsection{Client architecure}
For this stage, the client application is a simple terminal application that reads commands from the terminal and use 
the API provided by \texttt{ClientLibrary}, which is responsible for communicating with the Legder Service. 
The system uses nodes and clients. For simplicity, we assume that the first $n$ ids correspond to the 
nodes and the remaining to the clients. This is done to simplify the communication as the clients and the 
nodes are aware of each other at the start of the system.
The clients can behave in arbitraty way without being able to crash the system.
However, each client is free to append anything to the ledger (but it will be
associated with client id, which provides some accountability).

\subsection{Notes on some design choices}

\subsubsection{Message signing}

Since we don't use digital signatures for point to point authentication, QBFT
implementation needs to sign the messages for them to be included in the justifications.
In particular, \texttt{PREPARE} and \texttt{ROUND-CHANGE} messages must be signed
by replicas before being dispatched to the network, so that another replica
can include those messages in justification that are piggy-backed in \texttt{ROUND-CHANGE}
and \texttt{PRE-PREPARE} messages.
It should be noted that special care must be taken when signing \texttt{ROUND-CHANGE}
messages and when building the \texttt{PRE-PREPARE} justification. The latter
must include a set of \texttt{ROUND-CHANGE} message. However, the latter can't 
inclue their respective justification, as it would make the overall message
$\mathcal{O}(n^2)$. For these reason, the justification for a \texttt{PRE-PREPARE}
is a list of \texttt{ROUND-CHANGE} and a list of \texttt{PREPARE} (and for this
reason, \texttt{ROUND-CHANGE} must be signed without the justification).

\subsubsection{Message processing parallelization}

The initial codebase tried to have full parallelization of message processing, but
in our opinion the existing approach was inefficient, overly complex and hard to
test. These last properties seem very detrimental to building a dependable system
as they most likely lead to bugs that compromise the system's properties. The 
provided code created a new Thread for each message, and immetially activated
one upon rule. Because upon rules must executed atomically, these were (at least
in part) synchronized methods, which defeated the purpose of parallelization.
At the end of the day, it was likely that thread creation would be the bottneck.
Furthermore, as the code could in principle be concurrent, a defensive approach
is taken - every map, list and variable are concurrent, which not only hurts
performance, but sometimes is not enough to ensure correctness. For these reasons,
we chose to have a Thread pool that handled the messages. Each instance upon
rule is inside a critical section. This could be a performance problem if messages
signing was done in the critical section. For this reason the implementation,
check all signatures before the upon rule is executed and signs messages
after exiting the upon rule.


% strategy to ensure protocol is O(n2), - dsa 
% approach to parallelization and locking - dsa

\end{document}
